{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface AK and research community papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow to get info from AK and research community papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from scripts.hf_scraper_script import HFPapersScraper\n",
    "from scripts.arxiv_utils import *\n",
    "from scripts.openalex_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a list of authors from the papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get list of authors from a list of papers on huggingface: [AK and community daily papers](https://huggingface.co/papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2411.04923 ['Shehan Munasinghe', 'Hanan Gani', 'Wenqi Zhu', 'Jiale Cao', 'Eric Xing', 'Fahad Shahbaz Khan', 'Salman Khan']\n",
      "2411.05000 ['Jonathan Roberts', 'Kai Han', 'Samuel Albanie']\n",
      "2411.04075 ['Chuhan Li', 'Ziyao Shangguan', 'Yilun Zhao', 'Deyuan Li', 'Yixin Liu', 'Arman Cohan']\n",
      "2411.04905 ['Siming Huang', 'Tianhao Cheng', 'Jason Klein Liu', 'Jiaran Hao', 'Liuyihan Song', 'Yang Xu', 'J. Yang', 'J. H. Liu', 'Chenchen Zhang', 'Linzheng Chai', 'Ruifeng Yuan', 'Zhaoxiang Zhang', 'Jie Fu', 'Qian Liu', 'Ge Zhang', 'Zili Wang', 'Yuan Qi', 'Yinghui Xu', 'Wei Chu']\n",
      "2411.04928 ['Wenqiang Sun', 'Shuo Chen', 'Fangfu Liu', 'Zilong Chen', 'Yueqi Duan', 'Jun Zhang', 'Yikai Wang']\n",
      "2411.04989 ['Koichi Namekata', 'Sherwin Bahmani', 'Ziyi Wu', 'Yash Kant', 'Igor Gilitschenski', 'David B. Lindell']\n",
      "2411.04709 ['Wenhao Wang', 'Yi Yang']\n",
      "2411.05003 ['David Junhao Zhang', 'Roni Paiss', 'Shiran Zada', 'Nikhil Karnad', 'David E. Jacobs', 'Yael Pritch', 'Inbar Mosseri', 'Mike Zheng Shou', 'Neal Wadhwa', 'Nataniel Ruiz']\n",
      "2411.04999 ['Peiqi Liu', 'Zhanqiu Guo', 'Mohit Warke', 'Soumith Chintala', 'Chris Paxton', 'Nur Muhammad Mahi Shafiullah', 'Lerrel Pinto']\n",
      "2411.04996 ['Weixin Liang', 'Lili Yu', 'Liang Luo', 'Srinivasan Iyer', 'Ning Dong', 'Chunting Zhou', 'Gargi Ghosh', 'Mike Lewis', 'Wen-tau Yih', 'Luke Zettlemoyer', 'Xi Victoria Lin']\n",
      "2411.04752 ['Aniket Deroy', 'Subhankar Maity']\n",
      "2411.04965 ['Hongyu Wang', 'Shuming Ma', 'Furu Wei']\n",
      "2411.04496 ['Young-Jun Lee', 'Dokyong Lee', 'Junyoung Youn', 'Kyeongjin Oh', 'Ho-Jin Choi']\n",
      "2411.05007 ['Muyang Li', 'Yujun Lin', 'Zhekai Zhang', 'Tianle Cai', 'Xiuyu Li', 'Junxian Guo', 'Enze Xie', 'Chenlin Meng', 'Jun-Yan Zhu', 'Song Han']\n",
      "2411.05001 ['David M. Chan', 'Rodolfo Corona', 'Joonyong Park', 'Cheol Jun Cho', 'Yutong Bai', 'Trevor Darrell']\n",
      "2411.04335 ['He-Yen Hsieh', 'Ziyun Li', 'Sai Qian Zhang', 'Wei-Te Mark Ting', 'Kao-Den Chang', 'Barbara De Salvo', 'Chiao Liu', 'H. T. Kung']\n",
      "2411.05005 ['Shuhong Zheng', 'Zhipeng Bao', 'Ruoyu Zhao', 'Martial Hebert', 'Yu-Xiong Wang']\n",
      "2411.04952 ['Jaemin Cho', 'Debanjan Mahata', 'Ozan Irsoy', 'Yujie He', 'Mohit Bansal']\n"
     ]
    }
   ],
   "source": [
    "# initialize Huggingface scraper class\n",
    "hf_papers_scraper = HFPapersScraper()\n",
    "\n",
    "# get arxiv ids from the links\n",
    "arxiv_ids = hf_papers_scraper.get_paper_ids()\n",
    "\n",
    "# Loop through the arxiv ids and get the authors through the arxiv API and XML parsing\n",
    "for arxiv_id in arxiv_ids:\n",
    "    xml_content = get_content_by_id(arxiv_id)\n",
    "    print(arxiv_id, extract_paper_info(xml_content)[0]['authors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get list of paper authors by topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: negative sampling\n",
      "[{'title': 'Entity Aware Negative Sampling with Auxiliary Loss of False Negative\\n  Prediction for Knowledge Graph Embedding', 'author': ['Sang-Hyun Je']}, {'title': 'Does Negative Sampling Matter? A Review with Insights into its Theory\\n  and Applications', 'author': ['Zhen Yang', 'Ming Ding', 'Tinglin Huang', 'Yukuo Cen', 'Junshuai Song', 'Bin Xu', 'Yuxiao Dong', 'Jie Tang']}, {'title': 'Bayesian Negative Sampling for Recommendation', 'author': ['Bin Liu', 'Bang Wang']}, {'title': 'Synthetic Hard Negative Samples for Contrastive Learning', 'author': ['Hengkui Dong', 'Xianzhong Long', 'Yun Li', 'Lei Chen']}, {'title': 'UFNRec: Utilizing False Negative Samples for Sequential Recommendation', 'author': ['Xiaoyang Liu', 'Chong Liu', 'Pinzheng Wang', 'Rongqin Zheng', 'Lixin Zhang', 'Leyu Lin', 'Zhijun Chen', 'Liangliang Fu']}, {'title': 'Rethinking Samples Selection for Contrastive Learning: Mining of\\n  Potential Samples', 'author': ['Hengkui Dong', 'Xianzhong Long', 'Yun Li']}, {'title': 'MixKG: Mixing for harder negative samples in knowledge graph', 'author': ['Feihu Che', 'Guohua Yang', 'Pengpeng Shao', 'Dawei Zhang', 'Jianhua Tao']}, {'title': 'From Overfitting to Robustness: Quantity, Quality, and Variety Oriented\\n  Negative Sample Selection in Graph Contrastive Learning', 'author': ['Adnan Ali', 'Jinlong Li', 'Huanhuan Chen', 'Ali Kashif Bashir']}, {'title': 'Augmented Negative Sampling for Collaborative Filtering', 'author': ['Yuhan Zhao', 'Rui Chen', 'Riwei Lai', 'Qilong Han', 'Hongtao Song', 'Li Chen']}, {'title': 'Rethinking InfoNCE: How Many Negative Samples Do You Need?', 'author': ['Chuhan Wu', 'Fangzhao Wu', 'Yongfeng Huang']}]\n",
      "Topic: llm\n",
      "[{'title': 'Large Language Models as Software Components: A Taxonomy for\\n  LLM-Integrated Applications', 'author': ['Irene Weber']}, {'title': 'Parrot: Efficient Serving of LLM-based Applications with Semantic\\n  Variable', 'author': ['Chaofan Lin', 'Zhenhua Han', 'Chengruidong Zhang', 'Yuqing Yang', 'Fan Yang', 'Chen Chen', 'Lili Qiu']}, {'title': 'A Survey of Large Language Models for Code: Evolution, Benchmarking, and\\n  Future Trends', 'author': ['Zibin Zheng', 'Kaiwen Ning', 'Yanlin Wang', 'Jingwen Zhang', 'Dewu Zheng', 'Mingxi Ye', 'Jiachi Chen']}, {'title': 'A Survey of Large Language Models on Generative Graph Analytics: Query,\\n  Learning, and Applications', 'author': ['Wenbo Shang', 'Xin Huang']}, {'title': \"TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for\\n  Time Series\", 'author': ['Chenxi Sun', 'Hongyan Li', 'Yaliang Li', 'Shenda Hong']}, {'title': 'From LLMs to LLM-based Agents for Software Engineering: A Survey of\\n  Current, Challenges and Future', 'author': ['Haolin Jin', 'Linghan Huang', 'Haipeng Cai', 'Jun Yan', 'Bo Li', 'Huaming Chen']}, {'title': 'Large Language Model Supply Chain: Open Problems From the Security\\n  Perspective', 'author': ['Qiang Hu', 'Xiaofei Xie', 'Sen Chen', 'Lei Ma']}, {'title': 'MEGAnno+: A Human-LLM Collaborative Annotation System', 'author': ['Hannah Kim', 'Kushan Mitra', 'Rafael Li Chen', 'Sajjadur Rahman', 'Dan Zhang']}, {'title': 'Why and When LLM-Based Assistants Can Go Wrong: Investigating the\\n  Effectiveness of Prompt-Based Interactions for Software Help-Seeking', 'author': ['Anjali Khurana', 'Hari Subramonyam', 'Parmit K Chilana']}, {'title': 'Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks:\\n  Explainable Metrics and Diverse Prompt Templates', 'author': ['Hui Wei', 'Shenghua He', 'Tian Xia', 'Andy Wong', 'Jingyang Lin', 'Mei Han']}]\n",
      "Topic: ['reinforcement learning', 'transformers']\n",
      "[{'title': 'Recurrence for vertex-reinforced random walks on Z with weak\\n  reinforcements', 'author': ['Arvind Singh']}, {'title': 'Large deviations in the reinforced random walk model on trees', 'author': ['Yu Zhang']}, {'title': 'Some Insights into Lifelong Reinforcement Learning Systems', 'author': ['Changjian Li']}, {'title': 'Counterexample-Guided Repair of Reinforcement Learning Systems Using\\n  Safety Critics', 'author': ['David Boetius', 'Stefan Leue']}, {'title': 'Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey', 'author': ['Ngan Le', 'Vidhiwar Singh Rathour', 'Kashu Yamazaki', 'Khoa Luu', 'Marios Savvides']}, {'title': 'Asymptotic properties of a multicolored random reinforced urn model with\\n  an application to multi-armed bandits', 'author': ['Li Yang', 'Jiang Hu', 'Jianghao Li', 'Zhidong Bai']}, {'title': 'Causal Reinforcement Learning: A Survey', 'author': ['Zhihong Deng', 'Jing Jiang', 'Guodong Long', 'Chengqi Zhang']}, {'title': 'Edge- and vertex-reinforced random walks with super-linear reinforcement\\n  on infinite graphs', 'author': ['Codina Cotar', 'Debleena Thacker']}, {'title': \"Memory-two strategies forming symmetric mutual reinforcement learning\\n  equilibrium in repeated prisoners' dilemma game\", 'author': ['Masahiko Ueda']}, {'title': 'Distributed Deep Reinforcement Learning: A Survey and A Multi-Player\\n  Multi-Agent Learning Toolbox', 'author': ['Qiyue Yin', 'Tongtong Yu', 'Shengqi Shen', 'Jun Yang', 'Meijing Zhao', 'Kaiqi Huang', 'Bin Liang', 'Liang Wang']}]\n"
     ]
    }
   ],
   "source": [
    "# Can pass a topic or a list of topics\n",
    "topic_list = [\"negative sampling\", \"llm\", [\"reinforcement learning\", \"transformers\"]]\n",
    "for topic in topic_list:\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print([({'title': paper['title'], 'author': paper['authors']}) for paper in extract_paper_info(get_content_by_topic(topic, filter_type='any'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. OpenAlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.openalex.org/works?filter=topics.id:https://openalex.org/T10906\n"
     ]
    }
   ],
   "source": [
    "topic_ids = get_topic_ids_by_keyword(\"artificial intelligence\")\n",
    "works = get_works_by_topic_id(topic_ids[0], print_url=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference',\n",
       "  ['Judea Pearl']),\n",
       " ('Bayesian Data Analysis',\n",
       "  ['Andrew Gelman',\n",
       "   'John B. Carlin',\n",
       "   'Hal S. Stern',\n",
       "   'David B. Dunson',\n",
       "   'Aki Vehtari',\n",
       "   'Donald B. Rubin'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(work['title'], [author['author']['display_name'] for author in work['authorships']]) for work in works[0:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get list of papers using author name from ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://export.arxiv.org/api/query?search_query=au%3Asutton_richard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'True Online Emphatic TD($λ$): Quick Reference and Implementation\\n  Guide',\n",
       "  'authors': ['Richard S. Sutton'],\n",
       "  'summary': 'This document is a guide to the implementation of true online emphatic\\nTD($\\\\lambda$), a model-free temporal-difference algorithm for learning to make\\nlong-term predictions which combines the emphasis idea (Sutton, Mahmood & White\\n2015) and the true-online idea (van Seijen & Sutton 2014). The setting used\\nhere includes linear function approximation, the possibility of off-policy\\ntraining, and all the generality of general value functions, as well as the\\nemphasis algorithm\\'s notion of \"interest\".'},\n",
       " {'title': 'A History of Meta-gradient: Gradient Methods for Meta-learning',\n",
       "  'authors': ['Richard S. Sutton'],\n",
       "  'summary': 'The history of meta-learning methods based on gradient descent is reviewed,\\nfocusing primarily on methods that adapt step-size (learning rate)\\nmeta-parameters.'},\n",
       " {'title': 'The Quest for a Common Model of the Intelligent Decision Maker',\n",
       "  'authors': ['Richard S. Sutton'],\n",
       "  'summary': 'The premise of the Multi-disciplinary Conference on Reinforcement Learning\\nand Decision Making is that multiple disciplines share an interest in\\ngoal-directed decision making over time. The idea of this paper is to sharpen\\nand deepen this premise by proposing a perspective on the decision maker that\\nis substantive and widely held across psychology, artificial intelligence,\\neconomics, control theory, and neuroscience, which I call the \"common model of\\nthe intelligent agent\". The common model does not include anything specific to\\nany organism, world, or application domain. The common model does include\\naspects of the decision maker\\'s interaction with its world (there must be input\\nand output, and a goal) and internal components of the decision maker (for\\nperception, decision-making, internal evaluation, and a world model). I\\nidentify these aspects and components, note that they are given different names\\nin different disciplines but refer essentially to the same ideas, and discuss\\nthe challenges and benefits of devising a neutral terminology that can be used\\nacross disciplines. It is time to recognize and build on the convergence of\\nmultiple diverse disciplines on a substantive common model of the intelligent\\nagent.'},\n",
       " {'title': 'Toward Efficient Gradient-Based Value Estimation',\n",
       "  'authors': ['Arsalan Sharifnassab', 'Richard Sutton'],\n",
       "  'summary': 'Gradient-based methods for value estimation in reinforcement learning have\\nfavorable stability properties, but they are typically much slower than\\nTemporal Difference (TD) learning methods. We study the root causes of this\\nslowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned\\nloss function in the sense that its Hessian has large condition-number. To\\nresolve the adverse effect of poor conditioning of MSBE on gradient based\\nmethods, we propose a low complexity batch-free proximal method that\\napproximately follows the Gauss-Newton direction and is asymptotically robust\\nto parameterization. Our main algorithm, called RANS, is efficient in the sense\\nthat it is significantly faster than the residual gradient methods while having\\nalmost the same computational complexity, and is competitive with TD on the\\nclassic problems that we tested.'},\n",
       " {'title': 'Multi-timescale Nexting in a Reinforcement Learning Robot',\n",
       "  'authors': ['Joseph Modayil', 'Adam White', 'Richard S. Sutton'],\n",
       "  'summary': 'The term \"nexting\" has been used by psychologists to refer to the propensity\\nof people and many other animals to continually predict what will happen next\\nin an immediate, local, and personal sense. The ability to \"next\" constitutes a\\nbasic kind of awareness and knowledge of one\\'s environment. In this paper we\\npresent results with a robot that learns to next in real time, predicting\\nthousands of features of the world\\'s state, including all sensory inputs, at\\ntimescales from 0.1 to 8 seconds. This was achieved by treating each state\\nfeature as a reward-like target and applying temporal-difference methods to\\nlearn a corresponding value function with a discount rate corresponding to the\\ntimescale. We show that two thousand predictions, each dependent on six\\nthousand state features, can be learned and updated online at better than 10Hz\\non a laptop computer, using the standard TD(lambda) algorithm with linear\\nfunction approximation. We show that this approach is efficient enough to be\\npractical, with most of the learning complete within 30 minutes. We also show\\nthat a single tile-coded feature representation suffices to accurately predict\\nmany different signals at a significant range of timescales. Finally, we show\\nthat the accuracy of our learned predictions compares favorably with the\\noptimal off-line solution.'},\n",
       " {'title': 'Off-Policy Actor-Critic',\n",
       "  'authors': ['Thomas Degris', 'Martha White', 'Richard S. Sutton'],\n",
       "  'summary': 'This paper presents the first actor-critic algorithm for off-policy\\nreinforcement learning. Our algorithm is online and incremental, and its\\nper-time-step complexity scales linearly with the number of learned weights.\\nPrevious work on actor-critic algorithms is limited to the on-policy setting\\nand does not take advantage of the recent advances in off-policy gradient\\ntemporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable\\na target policy to be learned while following and obtaining data from another\\n(behavior) policy. For many problems, however, actor-critic methods are more\\npractical than action value methods (like Greedy-GQ) because they explicitly\\nrepresent the policy; consequently, the policy can be stochastic and utilize a\\nlarge action space. In this paper, we illustrate how to practically combine the\\ngenerality and learning potential of off-policy learning with the flexibility\\nin action selection given by actor-critic methods. We derive an incremental,\\nlinear time and space complexity algorithm that includes eligibility traces,\\nprove convergence under assumptions similar to previous off-policy algorithms,\\nand empirically show better or comparable performance to existing algorithms on\\nstandard reinforcement-learning benchmark problems.'},\n",
       " {'title': 'Scaling Life-long Off-policy Learning',\n",
       "  'authors': ['Adam White', 'Joseph Modayil', 'Richard S. Sutton'],\n",
       "  'summary': \"We pursue a life-long learning approach to artificial intelligence that makes\\nextensive use of reinforcement learning algorithms. We build on our prior work\\nwith general value functions (GVFs) and the Horde architecture. GVFs have been\\nshown able to represent a wide variety of facts about the world's dynamics that\\nmay be useful to a long-lived agent (Sutton et al. 2011). We have also\\npreviously shown scaling - that thousands of on-policy GVFs can be learned\\naccurately in real-time on a mobile robot (Modayil, White & Sutton 2011). That\\nwork was limited in that it learned about only one policy at a time, whereas\\nthe greatest potential benefits of life-long learning come from learning about\\nmany policies in parallel, as we explore in this paper. Many new challenges\\narise in this off-policy learning setting. To deal with convergence and\\nefficiency challenges, we utilize the recently introduced GTD({\\\\lambda})\\nalgorithm. We show that GTD({\\\\lambda}) with tile coding can simultaneously\\nlearn hundreds of predictions for five simple target policies while following a\\nsingle random behavior policy, assessing accuracy with interspersed on-policy\\ntests. To escape the need for the tests, which preclude further scaling, we\\nintroduce and empirically vali- date two online estimators of the off-policy\\nobjective (MSPBE). Finally, we use the more efficient of the two estimators to\\ndemonstrate off-policy learning at scale - the learning of value functions for\\none thousand policies in real time on a physical robot. This ability\\nconstitutes a significant step towards scaling life-long off-policy learning.\"},\n",
       " {'title': 'Planning by Prioritized Sweeping with Small Backups',\n",
       "  'authors': ['Harm van Seijen', 'Richard S. Sutton'],\n",
       "  'summary': 'Efficient planning plays a crucial role in model-based reinforcement\\nlearning. Traditionally, the main planning operation is a full backup based on\\nthe current estimates of the successor states. Consequently, its computation\\ntime is proportional to the number of successor states. In this paper, we\\nintroduce a new planning backup that uses only the current value of a single\\nsuccessor state and has a computation time independent of the number of\\nsuccessor states. This new backup, which we call a small backup, opens the door\\nto a new class of model-based reinforcement learning methods that exhibit much\\nfiner control over their planning process than traditional methods. We\\nempirically demonstrate that this increased flexibility allows for more\\nefficient planning by showing that an implementation of prioritized sweeping\\nbased on small backups achieves a substantial performance improvement over\\nclassical implementations.'},\n",
       " {'title': 'Temporal-Difference Networks',\n",
       "  'authors': ['Richard S. Sutton', 'Brian Tanner'],\n",
       "  'summary': 'We introduce a generalization of temporal-difference (TD) learning to\\nnetworks of interrelated predictions. Rather than relating a single prediction\\nto itself at a later time, as in conventional TD methods, a TD network relates\\neach prediction in a set of predictions to other predictions in the set at a\\nlater time. TD networks can represent and apply TD learning to a much wider\\nclass of predictions than has previously been possible. Using a random-walk\\nexample, we show that these networks can be used to learn to predict by a fixed\\ninterval, which is not possible with conventional TD methods. Secondly, we show\\nthat if the inter-predictive relationships are made conditional on action, then\\nthe usual learning-efficiency advantage of TD methods over Monte Carlo\\n(supervised learning) methods becomes particularly pronounced. Thirdly, we\\ndemonstrate that TD networks can learn predictive state representations that\\nenable exact solution of a non-Markov problem. A very broad range of\\ninter-predictive temporal relationships can be expressed in these networks.\\nOverall we argue that TD networks represent a substantial extension of the\\nabilities of TD methods and bring us closer to the goal of representing world\\nknowledge in entirely predictive, grounded terms.'},\n",
       " {'title': 'Learning to Predict Independent of Span',\n",
       "  'authors': ['Hado van Hasselt', 'Richard S. Sutton'],\n",
       "  'summary': \"We consider how to learn multi-step predictions efficiently. Conventional\\nalgorithms wait until observing actual outcomes before performing the\\ncomputations to update their predictions. If predictions are made at a high\\nrate or span over a large amount of time, substantial computation can be\\nrequired to store all relevant observations and to update all predictions when\\nthe outcome is finally observed. We show that the exact same predictions can be\\nlearned in a much more computationally congenial way, with uniform per-step\\ncomputation that does not depend on the span of the predictions. We apply this\\nidea to various settings of increasing generality, repeatedly adding desired\\nproperties and each time deriving an equivalent span-independent algorithm for\\nthe conventional algorithm that satisfies these desiderata. Interestingly,\\nalong the way several known algorithmic constructs emerge spontaneously from\\nour derivations, including dutch eligibility traces, temporal difference\\nerrors, and averaging. This allows us to link these constructs one-to-one to\\nthe corresponding desiderata, unambiguously connecting the `how' to the `why'.\\nEach step, we make sure that the derived algorithm subsumes the previous\\nalgorithms, thereby retaining their properties. Ultimately we arrive at a\\nsingle general temporal-difference algorithm that is applicable to the full\\nsetting of reinforcement learning.\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_name = \"richard sutton\"\n",
    "extract_paper_info(get_content_by_author(format_author_name(author_name), \n",
    "                                                      print_url=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "author-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
