{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add at the start of your notebook\n",
    "# import sys\n",
    "# import os\n",
    "\n",
    "# # Get absolute path to parent directory\n",
    "# parent_dir = os.path.dirname(os.getcwd())\n",
    "# sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.hf_scraper_script.py import HFPaperScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_urls(date=None):\n",
    "    \n",
    "    base_url = \"https://huggingface.co/papers\"\n",
    "    \n",
    "    if date:\n",
    "        url = f\"{base_url}?date={date}\"\n",
    "    else:\n",
    "        url = base_url\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    paper_links = soup.find_all('a', href=lambda x: x and '/papers/' in x)\n",
    "    \n",
    "    paper_urls = set()\n",
    "    for link in paper_links:\n",
    "        full_url = f\"https://huggingface.co{link['href']}\"\n",
    "        paper_urls.add(full_url)\n",
    "    \n",
    "    return list(paper_urls)\n",
    "\n",
    "paper_urls = get_paper_urls()\n",
    "\n",
    "arxiv_paper_links = []\n",
    "\n",
    "for url in paper_urls:\n",
    "    # print(url)\n",
    "    paper_response = requests.get(url)\n",
    "    paper_soup = BeautifulSoup(paper_response.text, 'html.parser')\n",
    "    paper_arxiv_link = paper_soup.find('a', href=lambda x: x and 'arxiv.org' in x)\n",
    "    arxiv_paper_links.append(paper_arxiv_link['href'])\n",
    "\n",
    "arxiv_paper_links = [link.replace('/abs/', '/pdf/') for link in arxiv_paper_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(arxiv_paper_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_content(pdf_url):\n",
    "    response = requests.get(pdf_url)\n",
    "    pdf = PyPDF2.PdfReader(BytesIO(response.content))\n",
    "    content = []\n",
    "    for page_num in range(len(pdf.pages)):\n",
    "        page = pdf.pages[page_num]\n",
    "        content.append(page.extract_text())\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_content = get_pdf_content(arxiv_paper_links[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preprint. Under review.\\nLEARNING VIDEO REPRESENTATIONS\\nWITHOUT NATURAL VIDEOS\\nXueyang Yu1Xinlei Chen2Yossi Gandelsman3\\n1ShanghaiTech University\\n2Meta AI\\n3University of California, Berkeley\\nABSTRACT\\nIn this paper, we show that useful video representations can be learned from\\nsynthetic videos and natural images, without incorporating natural videos in the\\ntraining. We propose a progression of video datasets synthesized by simple genera-\\ntive processes, that model a growing set of natural video properties (e.g. motion,\\nacceleration, and shape transformations). The downstream performance of video\\nmodels pre-trained on these generated datasets gradually increases with the dataset\\nprogression. A VideoMAE model pre-trained on our synthetic videos closes 97.2%\\nof the performance gap on UCF101 action classification between training from\\nscratch and self-supervised pre-training from natural videos, and outperforms\\nthe pre-trained model on HMDB51. Introducing crops of static images to the\\npre-training stage results in similar performance to UCF101 pre-training and out-\\nperforms the UCF101 pre-trained model on 11 out of 14 out-of-distribution datasets\\nof UCF101-P. Analyzing the low-level properties of the datasets, we identify corre-\\nlations between frame diversity, frame similarity to natural data, and downstream\\nperformance. Our approach provides a more controllable and transparent alternative\\nto video data curation processes for pre-training1.\\n1 I NTRODUCTION\\nLarge-scale data is a fundamental component for training neural networks in various domains, such\\nas natural language processing (NLP). To learn from such data, a prevalent technique is to pre-\\ntrain models via a self-supervised task (e.g. masked modeling (Devlin et al., 2019) or next-token\\nprediction (Radford et al., 2018; Brown et al., 2020). Adapting these models to downstream tasks\\nusually improves various NLP tasks.\\nWhile self-supervised pre-training is successful in NLP, the same success level has not yet been\\nachieved in computer vision. Specifically, in the video domain, although various large-scale datasets\\nexist and have been incorporated via similar self-supervised learning tasks, the improvements in\\ndownstream performance on video understanding (e.g. action recognition) are relatively low.\\nOne hypothesis for the limited success of self-supervised learning from videos is that current methods\\nfail to effectively utilize the natural video data and learn useful video representations from it. To\\ninvestigate this hypothesis, we ask if natural videos are even needed to learn video representations\\nthat are similar in performance to current state-of-the-art representations.\\nIn this work, we reach a downstream performance that is similar to the performance of models\\npre-trained on natural videos while pre-training solely on simple synthetic videos and static images.\\nWe propose a progression of simple synthetic video generators that model a gradually growing set of\\nvideo data properties - starting from static frames with solid-color circles and introducing additional\\nshapes, dynamics, temporal shape changes, acceleration, and other textures). We show that adding\\neach of the different properties improves the downstream video understanding performance.\\nSurprisingly, we find that the gap between the performance of our models and models that were\\npre-trained on natural videos is minor when we pre-train using purely synthetic data, and eliminated\\nwhen we introduce natural image crops. By pre-training a VideoMAE (Wang et al., 2023) on purely\\n1Project page, code, and models: https://unicorn53547.github.io/video_syn_rep/\\n1arXiv:2410.24213v1  [cs.CV]  31 Oct 2024'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_content[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "author-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
